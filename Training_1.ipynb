{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_893/95880468.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n",
      "2024-03-07 13:09:46.200536: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-07 13:09:46.200672: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-07 13:09:46.817451: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-07 13:09:48.322779: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-07 13:09:52.775796: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(csv_path):\n",
    "    df = pd.read_csv(csv_path, sep= ';')\n",
    "    required_columns = ['Azimuth', 'Altitude', 'Timestamp']\n",
    "\n",
    "    if not all (col in df.columns for col in required_columns):\n",
    "        raise ValueError(\"Required columns are not present in the DataFrame.\")\n",
    "    \n",
    "    X = df[['Timestamp','Azimuth','Altitude']].values\n",
    "    y = df[['Azimuth','Altitude']].values\n",
    "\n",
    "    return X, y, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = 'cleaned/cleaned_data_2014-2024.csv'\n",
    "X, y, df = load_dataset(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2 , random_state= 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "y_train_scaled = scaler_y.fit_transform(y_train)\n",
    "X_test_scaled = scaler_X.transform(X_test)\n",
    "y_test_scaled = scaler_y.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = 1\n",
    "X_train_reshaped = X_train_scaled.reshape((X_train_scaled.shape[0], timesteps, X_train_scaled.shape[1]))\n",
    "X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], timesteps, X_test_scaled.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-07 13:10:01.599146: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-07 13:10:03.732344: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-07 13:10:03.732601: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-07 13:10:03.737139: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-07 13:10:03.737284: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-07 13:10:03.737333: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-07 13:10:04.055354: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-07 13:10:04.055460: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-07 13:10:04.055473: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-03-07 13:10:04.055530: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-07 13:10:04.055552: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4080 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-07 13:10:07.590283: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-07 13:10:18.858990: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fe44d339cd0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-03-07 13:10:18.859120: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1660 Ti with Max-Q Design, Compute Capability 7.5\n",
      "2024-03-07 13:10:18.992442: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-03-07 13:10:19.294496: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1709791819.507767    1210 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 26s 120ms/step - loss: 0.3153 - mae: 0.4944 - val_loss: 0.3090 - val_mae: 0.4881\n",
      "Epoch 2/100\n",
      "103/103 [==============================] - 11s 104ms/step - loss: 0.3027 - mae: 0.4816 - val_loss: 0.2962 - val_mae: 0.4749\n",
      "Epoch 3/100\n",
      "103/103 [==============================] - 11s 104ms/step - loss: 0.2895 - mae: 0.4678 - val_loss: 0.2824 - val_mae: 0.4603\n",
      "Epoch 4/100\n",
      "103/103 [==============================] - 10s 99ms/step - loss: 0.2744 - mae: 0.4517 - val_loss: 0.2640 - val_mae: 0.4405\n",
      "Epoch 5/100\n",
      "103/103 [==============================] - 10s 99ms/step - loss: 0.1255 - mae: 0.2739 - val_loss: 0.0461 - val_mae: 0.1725\n",
      "Epoch 6/100\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.0390 - mae: 0.1620 - val_loss: 0.0368 - val_mae: 0.1583\n",
      "Epoch 7/100\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.0365 - mae: 0.1572 - val_loss: 0.0361 - val_mae: 0.1557\n",
      "Epoch 8/100\n",
      "103/103 [==============================] - 10s 102ms/step - loss: 0.0359 - mae: 0.1547 - val_loss: 0.0356 - val_mae: 0.1534\n",
      "Epoch 9/100\n",
      "103/103 [==============================] - 10s 102ms/step - loss: 0.0355 - mae: 0.1527 - val_loss: 0.0353 - val_mae: 0.1519\n",
      "Epoch 10/100\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.0353 - mae: 0.1515 - val_loss: 0.0351 - val_mae: 0.1509\n",
      "Epoch 11/100\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.0351 - mae: 0.1506 - val_loss: 0.0350 - val_mae: 0.1500\n",
      "Epoch 12/100\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.0350 - mae: 0.1496 - val_loss: 0.0348 - val_mae: 0.1490\n",
      "Epoch 13/100\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.0348 - mae: 0.1485 - val_loss: 0.0346 - val_mae: 0.1475\n",
      "Epoch 14/100\n",
      "103/103 [==============================] - 11s 102ms/step - loss: 0.0346 - mae: 0.1467 - val_loss: 0.0343 - val_mae: 0.1451\n",
      "Epoch 15/100\n",
      "103/103 [==============================] - 11s 103ms/step - loss: 0.0342 - mae: 0.1434 - val_loss: 0.0337 - val_mae: 0.1407\n",
      "Epoch 16/100\n",
      "103/103 [==============================] - 11s 102ms/step - loss: 0.0332 - mae: 0.1360 - val_loss: 0.0323 - val_mae: 0.1299\n",
      "Epoch 17/100\n",
      "103/103 [==============================] - 11s 103ms/step - loss: 0.0302 - mae: 0.1267 - val_loss: 0.0243 - val_mae: 0.1186\n",
      "Epoch 18/100\n",
      "103/103 [==============================] - 11s 103ms/step - loss: 0.0082 - mae: 0.0609 - val_loss: 0.0021 - val_mae: 0.0315\n",
      "Epoch 19/100\n",
      "103/103 [==============================] - 11s 103ms/step - loss: 0.0014 - mae: 0.0249 - val_loss: 9.3244e-04 - val_mae: 0.0203\n",
      "Epoch 20/100\n",
      "103/103 [==============================] - 11s 103ms/step - loss: 7.1841e-04 - mae: 0.0173 - val_loss: 5.4361e-04 - val_mae: 0.0144\n",
      "Epoch 21/100\n",
      "103/103 [==============================] - 11s 102ms/step - loss: 4.3799e-04 - mae: 0.0130 - val_loss: 3.7625e-04 - val_mae: 0.0127\n",
      "Epoch 22/100\n",
      "103/103 [==============================] - 11s 102ms/step - loss: 2.7591e-04 - mae: 0.0103 - val_loss: 2.0068e-04 - val_mae: 0.0084\n",
      "Epoch 23/100\n",
      "103/103 [==============================] - 11s 104ms/step - loss: 1.7074e-04 - mae: 0.0080 - val_loss: 1.3591e-04 - val_mae: 0.0070\n",
      "Epoch 24/100\n",
      "103/103 [==============================] - 11s 103ms/step - loss: 1.1440e-04 - mae: 0.0063 - val_loss: 9.6523e-05 - val_mae: 0.0056\n",
      "Epoch 25/100\n",
      "103/103 [==============================] - 11s 103ms/step - loss: 9.0219e-05 - mae: 0.0057 - val_loss: 7.4774e-05 - val_mae: 0.0048\n",
      "Epoch 26/100\n",
      "103/103 [==============================] - 11s 103ms/step - loss: 8.2334e-05 - mae: 0.0056 - val_loss: 6.1078e-05 - val_mae: 0.0042\n",
      "Epoch 27/100\n",
      "103/103 [==============================] - 11s 103ms/step - loss: 5.9523e-05 - mae: 0.0044 - val_loss: 6.2121e-05 - val_mae: 0.0049\n",
      "Epoch 28/100\n",
      "103/103 [==============================] - 11s 102ms/step - loss: 5.3086e-05 - mae: 0.0042 - val_loss: 4.8074e-05 - val_mae: 0.0038\n",
      "Epoch 29/100\n",
      "103/103 [==============================] - 11s 102ms/step - loss: 5.1911e-05 - mae: 0.0042 - val_loss: 4.2003e-05 - val_mae: 0.0035\n",
      "Epoch 30/100\n",
      "103/103 [==============================] - 11s 103ms/step - loss: 4.6854e-05 - mae: 0.0040 - val_loss: 3.7112e-05 - val_mae: 0.0031\n",
      "Epoch 31/100\n",
      "103/103 [==============================] - 11s 103ms/step - loss: 4.8813e-05 - mae: 0.0041 - val_loss: 4.2979e-05 - val_mae: 0.0041\n",
      "Epoch 32/100\n",
      "103/103 [==============================] - 11s 103ms/step - loss: 3.4271e-05 - mae: 0.0031 - val_loss: 3.2002e-05 - val_mae: 0.0029\n",
      "Epoch 33/100\n",
      "103/103 [==============================] - 11s 103ms/step - loss: 4.6903e-05 - mae: 0.0042 - val_loss: 3.4364e-05 - val_mae: 0.0034\n",
      "Epoch 34/100\n",
      "103/103 [==============================] - 11s 103ms/step - loss: 2.9613e-05 - mae: 0.0028 - val_loss: 3.5065e-05 - val_mae: 0.0036\n",
      "Epoch 35/100\n",
      "103/103 [==============================] - 11s 103ms/step - loss: 3.8552e-05 - mae: 0.0039 - val_loss: 3.2158e-05 - val_mae: 0.0034\n",
      "Epoch 36/100\n",
      "103/103 [==============================] - 11s 103ms/step - loss: 3.8377e-05 - mae: 0.0037 - val_loss: 2.5838e-05 - val_mae: 0.0026\n",
      "Epoch 37/100\n",
      "103/103 [==============================] - 11s 103ms/step - loss: 2.6649e-05 - mae: 0.0028 - val_loss: 2.4964e-05 - val_mae: 0.0025\n",
      "Epoch 38/100\n",
      "103/103 [==============================] - 11s 103ms/step - loss: 3.4309e-05 - mae: 0.0037 - val_loss: 2.7815e-05 - val_mae: 0.0031\n",
      "Epoch 39/100\n",
      "103/103 [==============================] - 10s 102ms/step - loss: 3.7672e-05 - mae: 0.0037 - val_loss: 2.8302e-05 - val_mae: 0.0032\n",
      "Epoch 40/100\n",
      "103/103 [==============================] - 11s 103ms/step - loss: 2.3247e-05 - mae: 0.0025 - val_loss: 2.4386e-05 - val_mae: 0.0028\n",
      "Epoch 41/100\n",
      "103/103 [==============================] - 11s 103ms/step - loss: 4.9310e-05 - mae: 0.0042 - val_loss: 3.5735e-05 - val_mae: 0.0041\n",
      "Epoch 42/100\n",
      "103/103 [==============================] - 11s 102ms/step - loss: 2.1455e-05 - mae: 0.0024 - val_loss: 2.0331e-05 - val_mae: 0.0022\n",
      "Epoch 43/100\n",
      "103/103 [==============================] - 11s 104ms/step - loss: 2.0159e-05 - mae: 0.0023 - val_loss: 2.0080e-05 - val_mae: 0.0023\n",
      "Epoch 44/100\n",
      "103/103 [==============================] - 11s 103ms/step - loss: 2.5516e-05 - mae: 0.0030 - val_loss: 1.8989e-05 - val_mae: 0.0021\n",
      "Epoch 45/100\n",
      "103/103 [==============================] - 11s 103ms/step - loss: 2.8696e-05 - mae: 0.0032 - val_loss: 2.3283e-05 - val_mae: 0.0029\n",
      "Epoch 46/100\n",
      "103/103 [==============================] - 11s 103ms/step - loss: 3.2902e-05 - mae: 0.0034 - val_loss: 1.7910e-05 - val_mae: 0.0021\n",
      "Epoch 47/100\n",
      "103/103 [==============================] - 11s 102ms/step - loss: 1.8779e-05 - mae: 0.0023 - val_loss: 1.7362e-05 - val_mae: 0.0020\n",
      "Epoch 48/100\n",
      "103/103 [==============================] - 10s 102ms/step - loss: 2.4107e-05 - mae: 0.0028 - val_loss: 1.5696e-04 - val_mae: 0.0106\n",
      "Epoch 49/100\n",
      "103/103 [==============================] - 11s 103ms/step - loss: 3.0833e-05 - mae: 0.0032 - val_loss: 1.7591e-05 - val_mae: 0.0022\n",
      "Epoch 50/100\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 2.7989e-05 - mae: 0.0029 - val_loss: 8.0691e-05 - val_mae: 0.0074\n",
      "Epoch 51/100\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 2.0513e-05 - mae: 0.0025 - val_loss: 1.6169e-05 - val_mae: 0.0020\n",
      "Epoch 52/100\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 2.0217e-05 - mae: 0.0026 - val_loss: 1.6180e-05 - val_mae: 0.0021\n",
      "Epoch 53/100\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 2.2250e-05 - mae: 0.0027 - val_loss: 1.4811e-05 - val_mae: 0.0018\n",
      "Epoch 54/100\n",
      "103/103 [==============================] - 10s 100ms/step - loss: 2.5107e-05 - mae: 0.0032 - val_loss: 1.9437e-05 - val_mae: 0.0027\n",
      "Epoch 55/100\n",
      "103/103 [==============================] - 10s 100ms/step - loss: 2.0068e-05 - mae: 0.0027 - val_loss: 2.5084e-05 - val_mae: 0.0035\n",
      "Epoch 56/100\n",
      "103/103 [==============================] - 10s 99ms/step - loss: 2.5521e-05 - mae: 0.0031 - val_loss: 1.4211e-05 - val_mae: 0.0019\n",
      "Epoch 57/100\n",
      "103/103 [==============================] - 10s 99ms/step - loss: 2.2674e-05 - mae: 0.0027 - val_loss: 1.5317e-05 - val_mae: 0.0021\n",
      "Epoch 58/100\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 1.5078e-05 - mae: 0.0021 - val_loss: 5.1657e-05 - val_mae: 0.0058\n",
      "Epoch 59/100\n",
      "103/103 [==============================] - 10s 100ms/step - loss: 2.8343e-05 - mae: 0.0029 - val_loss: 1.3010e-05 - val_mae: 0.0018\n",
      "Epoch 60/100\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 2.0140e-05 - mae: 0.0024 - val_loss: 1.7801e-05 - val_mae: 0.0027\n",
      "Epoch 61/100\n",
      "103/103 [==============================] - 10s 102ms/step - loss: 1.7440e-05 - mae: 0.0023 - val_loss: 1.3837e-05 - val_mae: 0.0020\n",
      "Epoch 62/100\n",
      "103/103 [==============================] - 11s 103ms/step - loss: 2.4238e-05 - mae: 0.0032 - val_loss: 1.5091e-05 - val_mae: 0.0023\n",
      "Epoch 63/100\n",
      "103/103 [==============================] - 10s 102ms/step - loss: 2.4624e-05 - mae: 0.0027 - val_loss: 4.0077e-05 - val_mae: 0.0050\n",
      "Epoch 64/100\n",
      "103/103 [==============================] - 11s 102ms/step - loss: 1.4694e-05 - mae: 0.0020 - val_loss: 1.1410e-05 - val_mae: 0.0016\n",
      "Epoch 65/100\n",
      "103/103 [==============================] - 11s 107ms/step - loss: 2.0271e-05 - mae: 0.0025 - val_loss: 1.6087e-05 - val_mae: 0.0025\n",
      "Epoch 66/100\n",
      "103/103 [==============================] - 10s 102ms/step - loss: 1.4510e-05 - mae: 0.0021 - val_loss: 1.7749e-05 - val_mae: 0.0028\n",
      "Epoch 67/100\n",
      "103/103 [==============================] - 10s 100ms/step - loss: 3.2373e-05 - mae: 0.0033 - val_loss: 1.1108e-05 - val_mae: 0.0016\n",
      "Epoch 68/100\n",
      "103/103 [==============================] - 10s 100ms/step - loss: 1.0700e-05 - mae: 0.0016 - val_loss: 1.0502e-05 - val_mae: 0.0015\n",
      "Epoch 69/100\n",
      "103/103 [==============================] - 11s 102ms/step - loss: 1.7856e-05 - mae: 0.0024 - val_loss: 2.9330e-05 - val_mae: 0.0042\n",
      "Epoch 70/100\n",
      "103/103 [==============================] - 10s 102ms/step - loss: 1.7129e-05 - mae: 0.0023 - val_loss: 8.2835e-05 - val_mae: 0.0077\n",
      "Epoch 71/100\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 1.4487e-05 - mae: 0.0021 - val_loss: 9.8802e-06 - val_mae: 0.0015\n",
      "Epoch 72/100\n",
      "103/103 [==============================] - 11s 103ms/step - loss: 1.9087e-05 - mae: 0.0024 - val_loss: 9.5926e-06 - val_mae: 0.0014\n",
      "Epoch 73/100\n",
      "103/103 [==============================] - 11s 102ms/step - loss: 2.3378e-05 - mae: 0.0027 - val_loss: 1.4544e-04 - val_mae: 0.0103\n",
      "Epoch 74/100\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 1.9029e-05 - mae: 0.0025 - val_loss: 9.5444e-06 - val_mae: 0.0015\n",
      "Epoch 75/100\n",
      "103/103 [==============================] - 11s 104ms/step - loss: 9.9539e-06 - mae: 0.0016 - val_loss: 1.4603e-05 - val_mae: 0.0025\n",
      "Epoch 76/100\n",
      "103/103 [==============================] - 10s 102ms/step - loss: 1.7493e-05 - mae: 0.0026 - val_loss: 1.0533e-05 - val_mae: 0.0018\n",
      "Epoch 77/100\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 3.3649e-05 - mae: 0.0033 - val_loss: 4.2703e-05 - val_mae: 0.0053\n",
      "Epoch 78/100\n",
      "103/103 [==============================] - 10s 102ms/step - loss: 1.0802e-05 - mae: 0.0017 - val_loss: 8.9339e-06 - val_mae: 0.0015\n",
      "Epoch 79/100\n",
      "103/103 [==============================] - 11s 103ms/step - loss: 8.4613e-06 - mae: 0.0014 - val_loss: 8.6407e-06 - val_mae: 0.0015\n",
      "Epoch 80/100\n",
      "103/103 [==============================] - 12s 119ms/step - loss: 1.8764e-05 - mae: 0.0027 - val_loss: 8.8009e-06 - val_mae: 0.0015\n",
      "Epoch 81/100\n",
      "103/103 [==============================] - 11s 108ms/step - loss: 8.6177e-06 - mae: 0.0015 - val_loss: 1.2109e-05 - val_mae: 0.0022\n",
      "Epoch 82/100\n",
      "103/103 [==============================] - 11s 104ms/step - loss: 1.6193e-05 - mae: 0.0026 - val_loss: 2.3315e-05 - val_mae: 0.0037\n",
      "Epoch 83/100\n",
      "103/103 [==============================] - 11s 104ms/step - loss: 1.5193e-05 - mae: 0.0023 - val_loss: 1.1262e-04 - val_mae: 0.0090\n",
      "Epoch 84/100\n",
      "103/103 [==============================] - 11s 104ms/step - loss: 1.9952e-05 - mae: 0.0024 - val_loss: 8.4850e-06 - val_mae: 0.0016\n",
      "Epoch 85/100\n",
      "103/103 [==============================] - 11s 105ms/step - loss: 1.8028e-05 - mae: 0.0024 - val_loss: 8.6768e-06 - val_mae: 0.0016\n",
      "Epoch 86/100\n",
      "103/103 [==============================] - 10s 102ms/step - loss: 8.5692e-06 - mae: 0.0016 - val_loss: 7.6206e-06 - val_mae: 0.0014\n",
      "Epoch 87/100\n",
      "103/103 [==============================] - 11s 102ms/step - loss: 1.4924e-05 - mae: 0.0024 - val_loss: 6.4915e-05 - val_mae: 0.0068\n",
      "Epoch 88/100\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 1.3550e-05 - mae: 0.0021 - val_loss: 6.8274e-06 - val_mae: 0.0012\n",
      "Epoch 89/100\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 1.6192e-05 - mae: 0.0024 - val_loss: 6.8384e-06 - val_mae: 0.0013\n",
      "Epoch 90/100\n",
      "103/103 [==============================] - 10s 102ms/step - loss: 1.4823e-05 - mae: 0.0024 - val_loss: 1.0011e-05 - val_mae: 0.0020\n",
      "Epoch 91/100\n",
      "103/103 [==============================] - 11s 102ms/step - loss: 2.0298e-05 - mae: 0.0027 - val_loss: 7.0877e-06 - val_mae: 0.0014\n",
      "Epoch 92/100\n",
      "103/103 [==============================] - 11s 103ms/step - loss: 6.7157e-06 - mae: 0.0013 - val_loss: 6.3837e-06 - val_mae: 0.0012\n",
      "Epoch 93/100\n",
      "103/103 [==============================] - 11s 102ms/step - loss: 1.2448e-05 - mae: 0.0022 - val_loss: 1.3118e-05 - val_mae: 0.0026\n",
      "Epoch 94/100\n",
      "103/103 [==============================] - 11s 102ms/step - loss: 1.5266e-05 - mae: 0.0025 - val_loss: 1.0952e-05 - val_mae: 0.0023\n",
      "Epoch 95/100\n",
      "103/103 [==============================] - 10s 99ms/step - loss: 1.5893e-05 - mae: 0.0026 - val_loss: 3.6053e-05 - val_mae: 0.0050\n",
      "Epoch 96/100\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 1.2999e-05 - mae: 0.0023 - val_loss: 1.1048e-05 - val_mae: 0.0023\n",
      "Epoch 97/100\n",
      "103/103 [==============================] - 10s 99ms/step - loss: 9.3242e-06 - mae: 0.0019 - val_loss: 7.8256e-06 - val_mae: 0.0017\n",
      "Epoch 98/100\n",
      "103/103 [==============================] - 10s 99ms/step - loss: 1.4155e-05 - mae: 0.0022 - val_loss: 5.6970e-06 - val_mae: 0.0012\n",
      "Epoch 99/100\n",
      "103/103 [==============================] - 10s 99ms/step - loss: 1.5416e-05 - mae: 0.0024 - val_loss: 5.7000e-06 - val_mae: 0.0012\n",
      "Epoch 100/100\n",
      "103/103 [==============================] - 10s 99ms/step - loss: 2.2680e-05 - mae: 0.0028 - val_loss: 2.2517e-05 - val_mae: 0.0038\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(units=1024, activation='relu', input_shape=(timesteps, X_train_scaled.shape[1]), return_sequences=True))\n",
    "model.add(LSTM(units=512, activation='relu', return_sequences=True))\n",
    "model.add(LSTM(units=256, activation='relu', return_sequences=True))\n",
    "model.add(LSTM(units=128, activation='relu', return_sequences=True))\n",
    "model.add(LSTM(units=64, activation='relu', return_sequences=True))\n",
    "model.add(LSTM(units=32, activation='relu', return_sequences=True))\n",
    "model.add(LSTM(units=16, activation='relu', return_sequences=True))\n",
    "model.add(LSTM(units=8, activation='relu'))\n",
    "model.add(Dense(units=2))\n",
    "\n",
    "optimizer = Adam(learning_rate=0.0001)\n",
    "model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])  \n",
    "\n",
    "history = model.fit(X_train_reshaped, y_train_scaled, epochs=100, batch_size=4096, validation_data=(X_test_reshaped, y_test_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3287/3287 [==============================] - 20s 6ms/step\n",
      "3287/3287 [==============================] - 24s 7ms/step - loss: 2.2517e-05 - mae: 0.0038\n"
     ]
    }
   ],
   "source": [
    "predicted_values_scaled = model.predict(X_test_reshaped)\n",
    "predicted_values = scaler_y.inverse_transform(predicted_values_scaled)\n",
    "mse = mean_squared_error(y_test_scaled, predicted_values_scaled)\n",
    "r2 = r2_score(y_test_scaled, predicted_values_scaled)\n",
    "\n",
    "accuracy = model.evaluate(X_test_reshaped, y_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps_test = X_test[:, 0]\n",
    "df_results = pd.DataFrame({\n",
    "    'Timestamp': timestamps_test,\n",
    "    'Actual_Azimuth': y_test[:, 0],\n",
    "    'Actual_Altitude': y_test[:, 1],\n",
    "    'Predicted_Azimuth': predicted_values[:, 0],\n",
    "    'Predicted_Altitude': predicted_values[:, 1]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_csv_path = 'azimuth_altitude_prediction_results.csv'\n",
    "df_results.to_csv(output_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error on Test Data: 2.2517023265190713e-05\n",
      "R-squared on Test Data: 0.9996852128883238\n",
      "Accuracy on Test Data: [2.2517018805956468e-05, 0.0037750808987766504]\n",
      "Results saved to: azimuth_altitude_prediction_results.csv\n"
     ]
    }
   ],
   "source": [
    "print(f\"Mean Squared Error on Test Data: {mse}\")\n",
    "print(f\"R-squared on Test Data: {r2}\")\n",
    "print(f\"Accuracy on Test Data: {accuracy}\")\n",
    "print(f\"Results saved to: {output_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save('model_save')\n",
    "#model.save_weights('model_weights.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
